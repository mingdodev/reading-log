## LLM과의 상호작용이 필요한 UI에서 사용자 경험을 높이기

사용자는 구체적인 내부 작업을 몰라야 한다. 그러나, 작업이 잘 진행되고 있다는 것은 알아야 한다. 기다림에 대한 불편을 제거해야 한다.

> 한국어 RAG 쓰기 피드백 시스템 설계 고민들에 적용해보자.

### 멀티 에이전트들의 응답 말투 통일시키기

- 어떤 곳에서는 하십시오체. 어떤 곳에서는 해요체. 이건 사용자가 간접적으로 내부 구조를 알게되는 것이다.
- 프롬프트 작성 시 해요체를 강제하는 것만으로 LLM 출력 통제가 완벽히 되지는 않았다.

→ 멀티 에이전트 구조라면, **응답 말투를 통일**하는 가이드라인 문서화가 효과적

다만 지금처럼 외부 LLM API에 의존하는 상황에서는 얼마나 효과가 있을지 미지수

### 멀티 에이전트 구조에서 스트리밍 응답은 필수

최종 사용자 응답은 다수의 LLM 응답을 조합해서 내려주기에, 초기 설계에서 **스트리밍 방식**과 **동기 일관 반환** 중 고민이 있었다. 그러나 내부적으로 작업을 병렬 실행하기에, 일반적인 문장 길이라면 평균 응답 시간이 13초를 넘지 않았다. 따라서 사용자가 충분히 이해 가능한 지연으로 판단, 로딩 화면에 인터랙션을 주고 따로 스트리밍 응답을 제공하지는 않았다.

- 하지만 테스트가 아닌 실제 사용 경험에서는 더 긴 지연이 발생할 수도 있다.
- 기다릴 수 있는 건 성공적인 응답이 보장될 때이다. 로딩 화면은 응답이 제대로 처리되고 있는지 확신하기 어렵다.

스트리밍 방식은 다음과 같은 사용자 경험을 제공하니, 고민해봐도 좋을 듯

- **AI의 사고 과정**을 시각적으로 보여줄 수 있다.
- 따라서 LLM 응답이 블랙박스가 아니다. **응답에 대한 신뢰**를 높일 수 있다.
